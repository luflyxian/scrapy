# !/usr/bin/python
# -*- coding: UTF-8 -*-
import threading
from concurrent.futures import ThreadPoolExecutor
from hashlib import md5
from pprint import pprint
import os
import requests
import redis
import time
import datetime
import pymysql
import threadpool
import random
from queue import Queue
import re
import hashlib
from lxml import etree
from threading import Lock
from functools import partial


lock = Lock()

Industry_dict = {} #商品列表，需要更新

Location_dict = {} #应用分类，需要更新


def parse_date(date_str):
    """正则解析日期"""
    try:
        days = re.search(r'(\d+)d', date_str).group(1)
        return datetime.date.today() - datetime.timedelta(days=int(days))
    except AttributeError:
        return datetime.date.today()


class SqlQueue(object):
    """mysql长连接类"""
    def __init__(self, host="localhost", user="root", password="123456", database="pa1"):
        self.host = host
        self.user = user
        self.password = password
        self.database = database
        self.charset = 'utf8'
        self.q = Queue(maxsize=3)
        for i in range(0, 3):
            db = pymysql.connect(host=host, user=user, password=password, database=database,
                                 charset='utf8')  # 打开数据库连接
            self.q.put(db)

    def test_conn(self, db):
        # noinspection PyBroadException
        try:
            db.ping()  # 运行ping函数，判断是否断开链接
            return True
        except BaseException:
            return False

    def select(self, sql, fetch_one=False):
        results = None
        # 获取数据链接
        while True:
            db = self.q.get()
            if db:
                break
            else:
                time.sleep(1)
        if not self.test_conn(db):
            db = pymysql.connect(host=self.host, user=self.user, password=self.password,
                                 database=self.database, charset=self.charset)  # 重连数据库
        cursor = db.cursor(pymysql.cursors.DictCursor)  # 使用cursor()方法获取操作游标
        try:
            cursor.execute(sql)  # 执行SQL语句
            if fetch_one:
                results = cursor.fetchone()
            else:
                results = cursor.fetchall()  # 获取所有记录列表
        except BaseException as e:
            print("Error: unable to fetch data", e)
        finally:
            self.q.put(db)
        return results

    def insert(self, sql, data=None):
        while True:
            db = self.q.get()
            if db:
                break
            else:
                time.sleep(1)
        if not self.test_conn(db):
            db = pymysql.connect(host=self.host, user=self.user, password=self.password,
                                 database=self.database, charset=self.charset)  # 重连数据库
        cursor = db.cursor(pymysql.cursors.DictCursor)  # 使用cursor()方法获取操作游标
        try:
            if data:
                cursor.execute(sql, data)  # 执行SQL语句
            else:
                cursor.execute(sql)  # 执行SQL语句
            db.commit()  # 提交到数据库执行
            # print('插入成功')
            return cursor.lastrowid
        except BaseException as e:
            print('Error: ', e)
            db.rollback()  # 发生错误时回滚
        finally:
            self.q.put(db)

    def __del__(self):
        try:
            while True:
                self.q.get().close()
                if self.q.empty():
                    break
        except BaseException as e:
            print(e)


class Spider:
    def __init__(self):
        self.spider_name = str(__file__.split('\\')[-1])  # 程序名称
        self.client = redis.Redis('10.86.0.107', decode_responses=True)  # 调用获取ip类
        self.headers = {
    "authority": "www.amazon.com",
    "accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "accept-language": "zh-CN,zh;q=0.9",
    "device-memory": "2",
    "downlink": "10",
    "dpr": "1",
    "ect": "4g",
    "referer": "https://www.amazon.com/s?bbn=7141123011&rh=n^%^3A11006702011&fs=true&ref=lp_11006702011_sar",
    "rtt": "0",
    "sec-ch-device-memory": "2",
    "sec-ch-dpr": "1",
    "sec-ch-ua": "^\\^Google",
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": "^\\^Windows^^",
    "sec-ch-ua-platform-version": "^\\^12.0.0^^",
    "sec-ch-viewport-width": "1436",
    "sec-fetch-dest": "document",
    "sec-fetch-mode": "navigate",
    "sec-fetch-site": "same-origin",
    "sec-fetch-user": "?1",
    "upgrade-insecure-requests": "1",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "viewport-width": "1436"
}
        self.sql = SqlQueue(host='10.86.0.107', user='root', password='lx123456', database='amazon')  # 调用操作sql类
        self.url_set = set()
        self.spider_start()
        self.error_num = 0
        self.error_recode_date = datetime.date.today()
        self.current_date = datetime.date.today()


    def get_proxy(self):
        """代理池"""
        while True:
            proxies = self.client.lrange('use_proxies', 0, -1)
            if proxies:
                return random.choice(proxies)
            else:
                print('未获取到代理ip,重试中...')
                time.sleep(3)

    def download(self, url, timesleep=0.3, timeout=45, ii=3):
        """下载网页，列表页正常request爬取，详情页使用curl_cffi调用ja3指纹爬取"""
        print('Downloading: %s' % url)
        proxy = self.get_proxy()  # 获取 代理ip
        cookies = {
    "session-id": "132-0457207-4788762",
    # "i18n-prefs": "USD",
    # "ubid-main": "131-6095297-7196553",
    # "session-id-time": "2082787201l",
    "lc-main": "en_US",
    # "av-timezone": "Asia/Shanghai",
    # "kndctr_7742037254C95E840A4C98A6_AdobeOrg_identity": "CiYwNTk2MjEyNDg4NDEzODIwMzMyMDYyNzUxMzE4NDIyOTAxNDM5NlIRCKHpqeLAMRgBKgRTR1AzMAHwAaHpqeLAMQ==",
    # "AMCV_7742037254C95E840A4C98A6^%^40AdobeOrg": "MCMID^|05962124884138203320627513184229014396",
    # "sp-cdn": "^\\^L5Z9:CN^^",
    # "session-token": "bhaD3rVEwfpQ9xxFfvj3ZgXYkqxS9brzNBCuVkPoFMk5L3sjZIa2FIkdkTThH0UplvuMzIFzQFIYHh1OG2PyX/u+oHYDVNnteEq0HVyT167QtUAqKpUiFwo7vK5hy2kgXpU7Ff73JzmOjLXt6YRkCsd1hdgMKfOzdlwv40WTeoXO7dnfq2Iu6WKidqqe/I4BwEf42oZmOMxgNS7FTihLmqVkytf3fVYifrA3drrdMuEIAxRxPlE9ANVo5assoyIjOrwuJGH4UfIlINfsY7kLtI7zZik9HgMsFQUf3OSgaqZHktRrlxINpHB4AgEimV3+EgMsNAZV4eESyefH4T9SWyWJ2moh4Ws9",
    # "csm-hit": "tb:9C143A1HFHD9KKWXX6M9+s-9C143A1HFHD9KKWXX6M9^|1701355243924&t:1701355243925&adb:adblk_no"
}
        proxies = { 'http': f"{proxy}"}
        if url in "/ref=":
            try:
                from curl_cffi import requests

                response = requests.get(url, headers=self.headers, proxies=proxies, timeout=timeout,impersonate="chrome101",
                                        cookies=cookies)  # 获取响应
                response.encoding = 'utf-8'  # 编码
                time.sleep(timesleep)
                return response.text
            except BaseException as e:
                print('error :', e)
                if ii > 0:
                    time.sleep(timesleep)
                    return self.download(url, timesleep=timesleep, timeout=timeout, ii=ii - 1)  # 失败重取
        try:
            response = requests.get(url, headers=self.headers, proxies=proxies, timeout=timeout,cookies=cookies)  # 获取响应
            response.encoding = 'utf-8'  # 编码
            time.sleep(timesleep)
            return response.text
        except BaseException as e:
            print('error :', e)
            if ii > 0:
                time.sleep(timesleep)
                return self.download(url, timesleep=timesleep, timeout=timeout, ii=ii - 1)  # 失败重取

    def get_list(self, url_dict):
        """列表页获取数据"""
        response_data = self.download(url_dict["url"])  # 爬取数据
        self.get_item_to_sql(response_data, url_dict)  # 列表第一页解析详情页url
        if not response_data:
            return
        html = etree.HTML(response_data)
        try:
            if html.xpath("//span[@class='mr-1']/text()"):
                page_total_str_re = html.xpath("//span[@class='mr-1']/text()")
                page_total_str_number_re = re.findall("\d+", str(page_total_str_re))  # 查询有多少职位
                page_total_str_number = ''.join(page_total_str_number_re)
                if page_total_str_number:  # 判断是否有翻页
                    page = int(int(page_total_str_number) / 10 + 2)
                    for i in range(2, page):
                        url = url_dict["url"]
                        if i>20:
                            return
                        url_next = url.split('?')[0] + '?page=' + f"{i}"
                        response = self.download(url_next)  # 开始从列表第二页爬取
                        if not response:
                            return
                        try:
                            self.get_item_to_sql(response, url_dict)
                        except Exception as e:
                            print(e)
                            print('问题点在翻页')
            else:
                print('翻页xpath修改')
        except Exception as e:
            print(e)

    def check(self):
        """设计监控检测"""
        try:
            lock.acquire()
            if self.error_num > 1000:

                mail_host = self.mail_host  # 设置服务器
                mail_user = self.mail_user  # 用户名
                mail_pass = self.mail_pass # 口令
                sender = self.sender #发送者
                receivers = self.receivers  # 接收邮件，可设置为你的QQ邮箱或者其他邮箱
                inpuit_Text = self.inpuit_Text #发送内容
                From = self.From #发送主题
                self.send_email(mail_host,mail_user,mail_pass,sender,receivers,inpuit_Text,From)

                # os._exit(0)

        finally:
            lock.release()


    def send_email(self,mail_host,mail_user,mail_pass,sender,receivers,inpuit_Text,From):
        """邮箱警告系统"""
        import smtplib
        from email.mime.text import MIMEText
        from email.header import Header

        # 第三方 SMTP 服务
        mail_host = mail_host  # 设置服务器
        mail_user = mail_user  # 用户名
        mail_pass = mail_pass  # 口令

        sender = sender
        receivers = [f'{receivers}']  # 接收邮件，可设置为你的QQ邮箱或者其他邮箱

        message = MIMEText(f'{inpuit_Text}', 'plain', 'utf-8')
        message['From'] = Header(f"{From}", 'utf-8')
        message['To'] = Header("测试", 'utf-8')

        subject = '爬虫程序报错'  # 标签
        message['Subject'] = Header(subject, 'utf-8')

        try:
            smtpObj = smtplib.SMTP()
            smtpObj.connect(mail_host, 25)  # 25 为 SMTP 端口号
            smtpObj.login(mail_user, mail_pass)
            smtpObj.sendmail(sender, receivers, message.as_string())
            print("邮箱发送成功")

        except smtplib.SMTPException:
            print("邮箱发送失败")


    def get_item_to_sql(self, response, url_dict):
        """
        获取详情页链接
        """
        if not response:
            return
        html = etree.HTML(response)
        try:
            if html.xpath(
                    "//div[@class='a flex p-3 border-b border-solid border-amazon-gray-200 cursor-pointer hover:bg-amazon-green-100 hover:border-amazon-green-100 md:border md:rounded-lg md:mb-4 md:p-4']"):
                data_list = html.xpath("//div[@class='a flex p-3 border-b border-solid border-amazon-gray-200 cursor-pointer hover:bg-amazon-green-100 hover:border-amazon-green-100 md:border md:rounded-lg md:mb-4 md:p-4']")
                for i in data_list:
                    jobName = i.xpath("./div[@class='w-full']/div[@class='flex items-start']/h2/a/text()")
                    detail_url_xpath = i.xpath("./div[@class='ui-logo-col flex-shrink-0 mr-3 md:mr-0 md:ml-4 md:order-last']/a/@href")
                    salary = i.xpath("./div[@class='w-full']/div[@class='text-sm md:text-base xl:flex xl:flex-wrap']/div[@class='font-semibold']/text()")

                    url_dict['jobName'] = ''.join(jobName).replace("  ", '').replace("\n", '')

                    remove_space = partial(re.compile('\s+').sub, ' ')
                    url_dict['jobName']=  remove_space(''.join(jobName)).strip()

                    url_dict['detail_url'] = '\n'.join(detail_url_xpath).replace("\n", '').replace(" ", '')
                    detail_url = url_dict['detail_url']
                    salary_if_ture = '\n'.join(salary).replace("\n", '').replace(" ", '')

                    url_dict['Salary'] = self.judge(salary_if_ture)

                    if "?" in detail_url:
                        detail_url = detail_url.split("?")[0]
                    response = self.download(detail_url)
                    if self.check_duplicates(detail_url):
                        print("重复链接",detail_url)
                        continue
                    self.get_jobinfo(response,detail_url, url_dict)  #


        except Exception as e:
            print('报错了，在这',e)

    def get_jobinfo(self, response, detail_url, url_dict):
        """
        抓取详情页 详情页数据获取
        :param item:
        :return:
        """
        item = url_dict.copy()
        html = etree.HTML(response)

        # jobName = html.xpath("//section/h1//text()")
        # item['jobName'] = ''.join(jobName)  # 职位
        item['publishTime'] = datetime.datetime.now().strftime('%Y-%m-%d')  # 发布时间
        item['update_time'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # 入库时间
        jobInfo = html.xpath("//section//text()")  # 岗位详情
        item['jobInfo'] = '\n'.join(jobInfo)

        item['publishTime'] = datetime.datetime.now().strftime('%Y-%m-%d')  # 发布时间
        item['update_time'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # 入库时间
        # item['detailUrl'] = detail_url  # 源网址
        jobInfo = html.xpath(
            "//div[@class='flex-grow']/section[@class='adp-body mx-4 mb-4 text-sm md:mx-0 md:text-base md:mb-0']//text()")  # 岗位详情
        item['jobInfo'] = '\n'.join(jobInfo)

        data_list = [' '.join(_.strip() for _ in x.xpath('.//text()') if _.strip()) for x in html.xpath('//table//tr')]

        item['detailUrl'] = detail_url  # 源网址
        item['city'] = url_dict["area"]  # 城市
        item['Industry'] = url_dict["Industry"]  # 行业类型
        ls = ['Company', 'Location']
        if item["Salary"] == '':
            Salary = html.xpath("//div[@class='p-3 border-b border-dotted border-amazon-gray-200']/div/text()")
            try:
                Salary_re = re.findall("\$(.*?)',", str(Salary))[0]
                item['Salary'] = "$"+Salary_re
            except:
                item['Salary']=''
            # item['job_type'] =''
        # item['Classification] = ''
        item.update({key: '' for key in ls})
        pat = re.compile('(.*?)\:(.*)')
        for data in data_list:
            if pat.search(data):
                item[pat.search(data).group(1)] = pat.search(data).group(2)
        item['company'] = item['Company']
        item['area'] = item['Location']
        pprint(item)

        start_time = datetime.datetime.strptime(str(datetime.datetime.now().date()) + '9:00', '%Y-%m-%d%H:%M')
        end_time = datetime.datetime.strptime(str(datetime.datetime.now().date()) + '17:00', '%Y-%m-%d%H:%M')
        now_time = datetime.datetime.now()
        # 方法一：
        if start_time < now_time < end_time:
            if not self.check_item(item):
                self.check()#结果为False则表示没有问题，运行这里
            self.insert(item) #结果为True则表示没有问题，运行这里
        else:
            self.insert(item)

    def insert(self, item):
        """
        数据入库
        :param item: 爬取的字段字典
        :return:
        """
        print(self.spider_name.center(100, '='))
        date_str = datetime.date.today().strftime('%Y%m')
        sql_job = "INSERT " + f"""INTO amazon_jobinfo_{date_str} (jobName,Salary,detailUrl,city,
    publishTime,company,Industry,area,update_time,jobInfo) 
    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);"""
        self.sql.insert(sql_job, [
            item['jobName'], item['Salary'], item['detailUrl'], item['city'], item['publishTime'],
            item['company'], item['Industry'], item['area'], item['update_time'], item['jobInfo']])

        self.sql.insert("INSERT INTO amazon_qc (url) VALUES (%s);", [item['detailUrl']])
        print('工作数据入库成功'.center(100, '#'))

    def check_duplicates(self, url):
        """加密url节省数据库空间"""
        md5_url = md5(url.encode()).hexdigest()  # md5
        if self.current_date.month == datetime.date.today().month:

            return md5_url in self.url_set
        else:
            self.url_set.clear()
            self.current_date = datetime.date.today()

    def add_url_set(self, url):
        """ md5转码"""
        self.url_set.add(md5(url.encode()).hexdigest())

    def spider_start(self):
        """ 获取数据库链接去重"""
        date_str = datetime.date.today().strftime('%Y%m')
        url_list = self.sql.select("SELECT " + f"MD5(detailUrl) as url FROM amazon_jobinfo_{date_str}")

        for url in url_list:
            self.url_set.add(url['url'])

def list_url():
    start_list = []
    for Location, Location_url in Location_dict.items():
        for Industry, Industry_url in Industry_dict.items():
            start_list.append({'area': Location,
                               'Industry': Industry,
                               'url': f'https://www.amazon.com/{Location_url}/{Industry_url}?page=1'})
    return start_list

def main():
    S = Spider()  # 获取爬虫类
    S.spider_start()
    url_dict = list_url()  # 列表页不同的关键词data参数
    url_dict.reverse()
    while True:  # 循环爬取
        pool = ThreadPoolExecutor(32)
        print("开始运行")
        print('列表页数量：', len(url_dict))
        res = pool.map(S.get_list, url_dict)
        for x in res:
            time.sleep(0.3)
            pass
        print('线程执行完毕,程序停止一小时')
        pool.shutdown()  # 线程停止
        time.sleep(360)


if __name__ == '__main__':
    main()
